{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. INSTALL & IMPORT NECESSARY PACKAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded pose_landmarker.task\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_heavy/float16/1/pose_landmarker_heavy.task'\n",
    "filename = 'pose_landmarker.task'\n",
    "\n",
    "response = requests.get(url)\n",
    "with open(filename, 'wb') as file:\n",
    "    file.write(response.content)\n",
    "print(f\"Downloaded {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mediapipe import solutions\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "profLandT = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "II.a. draw_landmarks_on_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These will draw the landmarks on a detect person, as well as the expected connections between those markers.\n",
    "\n",
    "\n",
    "def draw_landmarks_on_image(rgb_image, detection_result):\n",
    "  pose_landmarks_list = detection_result.pose_landmarks\n",
    "  annotated_image = np.copy(rgb_image)\n",
    "\n",
    "  # Loop through the detected poses to visualize.\n",
    "  for idx in range(len(pose_landmarks_list)):\n",
    "    pose_landmarks = pose_landmarks_list[idx]\n",
    "\n",
    "    # Draw the pose landmarks.\n",
    "    pose_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "    pose_landmarks_proto.landmark.extend([\n",
    "      landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in pose_landmarks\n",
    "    ])\n",
    "    solutions.drawing_utils.draw_landmarks(\n",
    "      annotated_image,\n",
    "      pose_landmarks_proto,\n",
    "      solutions.pose.POSE_CONNECTIONS,\n",
    "      solutions.drawing_styles.get_default_pose_landmarks_style())\n",
    "  return annotated_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "II.b. save_landmarks_and_timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_landmarks_and_timestamps(detection_result, timestamp_ms):\n",
    "    global profLandT\n",
    "    # with open(filename, 'a') as file:\n",
    "    for pose_landmarks in detection_result.pose_landmarks:\n",
    "        landmarks_data = []\n",
    "        for landmark in pose_landmarks:\n",
    "            landmarks_data.append((landmark.x, landmark.y, landmark.z))\n",
    "        profLandT.append([timestamp_ms, landmarks_data])\n",
    "            # file.write(f\"{timestamp_ms} {landmarks_data}\\n\")\n",
    "\n",
    "    return profLandT\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(input_video_path, output_video_path):\n",
    "    # STEP 1: Import the necessary modules.\n",
    "    mp_pose = mp.solutions.pose\n",
    "\n",
    "    # STEP 2: Create an PoseLandmarker object.\n",
    "    base_options = python.BaseOptions(model_asset_path='pose_landmarker.task')\n",
    "    options = vision.PoseLandmarkerOptions(\n",
    "        running_mode=mp.tasks.vision.RunningMode.VIDEO,\n",
    "        base_options=base_options,\n",
    "        output_segmentation_masks=True)\n",
    "    detector = vision.PoseLandmarker.create_from_options(options)\n",
    "\n",
    "    # Load the input Video\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "    # Load the frame rate of the video using OpenCV’s CV_CAP_PROP_FPS\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    # Define the codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    frame_count = 0\n",
    "\n",
    "    # Loop through each frame in the video using VideoCapture#read()\n",
    "    while cap.isOpened():\n",
    "        # Read a single frame from the video\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        # Check if frame reading was successful\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Convert the frame received from OpenCV to a MediaPipe’s Image object.\n",
    "        numpy_frame_from_opencv = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=numpy_frame_from_opencv)\n",
    "\n",
    "        # Detect pose landmarks from the input image.\n",
    "        timestamp_ms = int((frame_count / fps) * 1000)  # Calculate the timestamp in milliseconds\n",
    "        detection_result = detector.detect_for_video(mp_image, timestamp_ms)\n",
    "        \n",
    "        # Save the detected landmarks and timestamps\n",
    "        save_landmarks_and_timestamps(detection_result, timestamp_ms)\n",
    "        \n",
    "        # Process the detection result. In this case, visualize it.\n",
    "        annotated_image = draw_landmarks_on_image(numpy_frame_from_opencv, detection_result)\n",
    "        \n",
    "        # Convert the annotated image back to BGR for OpenCV\n",
    "        output_frame = cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Write the frame to the output video\n",
    "        out.write(output_frame)\n",
    "        \n",
    "        # Display the resulting frame\n",
    "        cv2.imshow('Pose Detection', output_frame)\n",
    "        \n",
    "        # Press 'q' on the keyboard to exit the loop\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "    # When everything done, release the video capture and writer objects\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_score_comparison(current_landmarks, reference_landmarks):\n",
    "    if not current_landmarks or not reference_landmarks:\n",
    "        return None\n",
    "\n",
    "    score = 0\n",
    "    total_distance = 0\n",
    "    num_points = len(current_landmarks)\n",
    "\n",
    "    for i in range(num_points):\n",
    "\n",
    "        x_diff = current_landmarks[0][i].x - reference_landmarks[i][0]\n",
    "        y_diff = current_landmarks[0][i].y - reference_landmarks[i][1]\n",
    "        z_diff = current_landmarks[0][i].z - reference_landmarks[i][2] \n",
    "        distance = (x_diff ** 2 + y_diff ** 2 + z_diff ** 2) ** 0.5\n",
    "        total_distance += distance\n",
    "    \n",
    "    # Normalize the score (e.g., assuming a max possible distance)\n",
    "    max_distance = num_points * (3**0.5)  # example normalization factor\n",
    "    score = max(0, 1 - (total_distance / max_distance))  # ensuring score doesn't go below 0\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_video(input_video_path, output_video_path):\n",
    "    # Load the saved landmarks and timestamps\n",
    "    # landmarks_data = load_landmarks_and_timestamps(landmarks_file_path)\n",
    "    landmarks_data = profLandT\n",
    "\n",
    "    base_options = python.BaseOptions(model_asset_path='pose_landmarker.task')\n",
    "    options = vision.PoseLandmarkerOptions(\n",
    "        running_mode=mp.tasks.vision.RunningMode.VIDEO,\n",
    "        base_options=base_options,\n",
    "        output_segmentation_masks=True)\n",
    "    detector = vision.PoseLandmarker.create_from_options(options)\n",
    "\n",
    "    # Load the input Video\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    # Define the codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    frame_count = 0\n",
    "\n",
    "    total_score = 0\n",
    "    num_frames = 0\n",
    "\n",
    "    # Helper function to find the closest landmarks\n",
    "    def find_closest_landmarks(timestamp):\n",
    "        closest_landmarks = None\n",
    "        min_diff = float('inf')\n",
    "        for ts, landmarks in landmarks_data:\n",
    "            diff = abs(ts - timestamp)\n",
    "            if diff < min_diff:\n",
    "                min_diff = diff\n",
    "                closest_landmarks = landmarks\n",
    "        return closest_landmarks if min_diff <= 50 else None  # Adjust threshold as needed\n",
    "\n",
    "    # Loop through each frame in the video\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        numpy_frame_from_opencv = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=numpy_frame_from_opencv)\n",
    "\n",
    "        current_frame_timestamp_ms = int((frame_count / fps) * 1000)\n",
    "        detection_result = detector.detect_for_video(mp_image, current_frame_timestamp_ms)\n",
    "\n",
    "        # Extract the detected landmarks\n",
    "        current_landmarks = detection_result.pose_landmarks\n",
    "\n",
    "\n",
    "        score = None\n",
    "        closest_landmarks = find_closest_landmarks(current_frame_timestamp_ms)\n",
    "        if closest_landmarks is not None:\n",
    "            score = compute_score_comparison(current_landmarks, closest_landmarks)\n",
    "            if score is not None:\n",
    "                total_score += score  # Accumulate the score\n",
    "                num_frames += 1\n",
    "\n",
    "        if score is not None:\n",
    "            score_text = f\"Score: {score * 100:.2f}%\"\n",
    "            cv2.putText(frame, score_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 4, cv2.LINE_AA)\n",
    "        else:\n",
    "            print(f\"No score computed for frame {frame_count} at timestamp {current_frame_timestamp_ms} ms\")\n",
    "\n",
    "        # Write the frame to the output video\n",
    "        out.write(frame)\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "    if num_frames > 0:\n",
    "        general_score = total_score / num_frames\n",
    "    else:\n",
    "        general_score = 0\n",
    "\n",
    "    print(f\"General Score for the entire video: {general_score * 100:.2f}%\")\n",
    "\n",
    "    # Release video capture and writer objects\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. LOAD PARAMETERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process Instructor Video "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_video(\"prof/guy-prof.mp4\", \"prof/output_prof-guy.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score Student Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General Score for the entire video: 67.11%\n"
     ]
    }
   ],
   "source": [
    "score_video(\"student/student-male01.mp4\", \"student/scored/male01-scored.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No score computed for frame 1665 at timestamp 55500 ms\n",
      "No score computed for frame 2286 at timestamp 76200 ms\n",
      "No score computed for frame 2289 at timestamp 76300 ms\n",
      "General Score for the entire video: 65.64%\n"
     ]
    }
   ],
   "source": [
    "#FALSE VIDEO\n",
    "score_video(\"false-vids/Maria.mp4\", \"false-vids/scored_Maria.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No score computed for frame 2138 at timestamp 71266 ms\n",
      "General Score for the entire video: 58.75%\n"
     ]
    }
   ],
   "source": [
    "# FALSE VIDEO\n",
    "score_video(\"false-vids/HipHop.mp4\", \"false-vids/scored_HipHop.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No score computed for frame 1236 at timestamp 41200 ms\n",
      "General Score for the entire video: 80.93%\n"
     ]
    }
   ],
   "source": [
    "score_video(\"student/student-male02.mp4\", \"student/scored/male02-scored.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FEMALE TIKLOS VID (female reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_video(\"prof/girl-prof.mp4\", \"prof/marked/girl-prof.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No score computed for frame 2430 at timestamp 81000 ms\n",
      "General Score for the entire video: 60.45%\n"
     ]
    }
   ],
   "source": [
    "score_video(\"student/student-female.mp4\", \"student/scored/female01.mp4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
